\documentclass{article}
\usepackage[vmargin=1in,hmargin=1in]{geometry}
\usepackage{picture,xcolor}


\begin{document}

<<include=FALSE>>=
opts_chunk$set(fig.align='center', cache=TRUE, warning=FALSE)
@


\begin{center}
\large \textsc{Homework 1 Solution} \\
STAT 4410/8416 Section 001/002\\
\textsc{Fall 2015}\\
Due: September 16, 2015 by midnight
\normalsize
\end{center}


\begin{enumerate}
\item \begin{enumerate} 
     \item What is data science? \\
     
{\bf Answer:} With the advent of new data technologies the term ``Data science'' is getting popular now a days. Many people explain the term from many perspectives. Some people even tag this with so called ``big data''. Many people try to explain data science using Venn diagrams. One of them, created by Drew Conway, has been very popular and got place in Wikipedia to describe data science. While this Venn diagram nicely portrays the skills required by data scientists, it does not define what data science is. In 2011 Dr. Cleveland, in his article ``Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics", first introduced data science as an independent discipline extending statistics. In all the explanations, the one missing piece is the data product. Taking it into consideration, we define data science as below; \\

``Data science is the art and science of transforming data into deliverable data product.'' \\

No matter how it is defined, it is agreed by all as we see in Wikipedia that it is a highly interdisciplinary field. It requires knowledge from various areas such as computing, statistics, mathematics and business. \\

     \item Explain with an example what you mean by data product.\\
     
{\bf Answer:} By data product we mean a data dependent product that has some commercial value. It could be a report based on data or a visualization tool that display interesting and useful information from the data. The data product could be another data set summarizing the original data set. For example, your credit score is a data product that is obtained from your credit history data.
     \item Carefully read the Cleveland's paper shown in lecture 2 and discuss what he suggested about the field of statistics and data science. \\
     
     {\bf Answer:} In the paper Dr. Cleveland presented a plan to extend the technical areas of statistics focusing on the data analysis. The plan sets out six technical areas and the percentage of resources to be allocated to them as we see in the following figure.
<<fig.height=3, out.width='.75\\linewidth',echo=FALSE>>=
library(ggplot2)
technical_area <- c("Multidisciplinary Investigations",
                    "Models and Methods for Data",
                    "Computing with Data",
                    "Pedagogy",
                    "Tool Evaluation",
                    "Theory")
resource_percent <- c(25,20,15,15,5,20) 
ds <- data.frame(technical_area,resource_percent)
ggplot(ds, aes(reorder(technical_area,resource_percent),resource_percent)) + 
  geom_bar(stat="identity", fill='gray') + coord_flip() +
  xlab("Technical areas") + ylab("Percentage of resources") +
  theme_bw()
@


Since this is a major change in statistics, he suggested that the evolving field should be called ``data science''. One outcome of this plan joins computer science and mathematics and suggests that both computing and mathematical skills are  the fundamental skills of data science.\\
     
     \item Explain in a short paragraph how data science is different from computer science.\\
     
{\bf Answer:} While there are intersections between computer science and data science, these two fields are much different in many ways. It is true that data science would not be in this form today without the existence of computer science. All the modern data technologies are provided through computer science. But computer science does not necessarily focus on analyzing data and creating data product. This requires additional knowledge on statistics, mathematics and business. The data science being highly interdisciplinary with the power of all those areas evolved as an independent discipline giving it a whole different outlook than computer science.

     \item What is data literacy? Is it important to be data literate in this modern world? Explain why or why not.\\

{\bf Answer:} Data literacy means basic ability of gleaning useful information from data. Now, to extract information we need to know about some data munging techniques, statistical tools as well as computing techniques. To understand whether that information is useful or not we need to realize business goals. Thus having basic knowledge on data munging, statistics, computing and business can give us data literacy.


\end{enumerate}  

\item The function \texttt{sqrt()} provides the square root of any number. However, it can't provide any square root of negative number. We want to create our own function to provide a message for negative number. 

\begin{enumerate} 

  \item Create a new \textbf{R} function \texttt{getRoot()} that will provide square root of any number. If the number is negative it should return `not possible'. Demonstrate your function such that it produces the following outputs.
  \begin{center}  getRoot(4) = 2, getRoot(-4) = `not possible' \end{center}

{\bf Answer:} 
<<>>=
getRoot <- function(x){
  if(x<0) 
    return('not possible') else 
      return(sqrt(x))
}
getRoot(4)
getRoot(-4)
@ 

\item Does your function produce expected results for vector input as well? For example does it give the following result? Explain why or why not.
 \begin{center}  getRoot(c(4,-4, 9, -16))=2 `not possible' 3 `not possible' \end{center}
 
{\bf Answer:} My function does not produce expected answer for vector input as demonstrated below. 
<<warning=FALSE>>=
getRoot(c(4,-4, 9, -16))
@
The reason is the `if' condition is not vectorised. The `if' only works on the first element of the vector. The rest of the elements are ignored. So, when second value in the vector is -4, it treats it a positive number and tries to compute square root initiating a warning message with usual answer which we did not expect.

\item If your function does not work as expected, how can you make it work properly?

{\bf Answer:} Since the function does not work properly with vector input, we need to apply our function on each element of vector. This can be done as follows.
<<warning=FALSE>>=
sapply(c(4,-4, 9, -16), getRoot)
@

\end{enumerate} 

\item In our \textbf{R} class we learnt about recursive functions that produce a sequence of numbers upto a given number say $n$ as demonstrated in the following codes.

<<>>=
foo <- function(x){
  print(x)
  if(x>1) foo(x-1)
}

moo <- function(x){
  if(x>1) moo(x-1)
  print(x)
}

foo(3)
moo(3)
@

Explain why function \texttt{moo()} prints 1 through 3 while function \texttt{foo()} prints from 3 through 1.

{\bf Answer:} Between two functions \texttt{foo()} and \texttt{moo()}, positions of the \texttt{print()} statement make diï¬€erence. In \texttt{foo()} the print statement executed before recursion and gives output 3,2,1. On the other hand \texttt{moo()} executes print statement after recursion meaning that it holds the printing untill all recurssions are done. Finally it starts printing backwards that it held in each itteration and gives result 1,2,3.


\item Write a program that will do the following. Include your codes and necessary outputs to demonstrate your work.

\begin{enumerate}
  \item \label{exp-hist} Generate 800000 random numbers from a gamma distribution with shape = 5, scale= 5 and store these numbers in a vector called myVector. \textbf{Report} a histogram of the numbers you just generated.
  
<<fig.width=5, fig.height=5, fig.align='center',out.width='.4\\linewidth'>>=
# Since reproducibility is expected I set the random seed first
# This will make sure you have same simulation every time you compile
set.seed(4536)
myVector <- rgamma(n=800000,shape=5, scale=5)
hist(myVector)
@
  
  \item Convert myVector into a matrix of 800 rows and assign it to an object called myMatrix. \textbf{Report} the dimension of myMatrix.
 <<>>=
myMatrix <- matrix(myVector, nrow=800)
dim(myMatrix)
@  
  
  \item \label{mean-hist} Compute the row means of myMatrix. \textbf{Report} a histogram of those row means.
  
<<fig.width=5, fig.height=5, fig.align='center',out.width='.4\\linewidth'>>=
rowmeans <- rowMeans(myMatrix)
hist(rowmeans)
@  
  \item Explain why the two histograms you have created in questions (\ref{exp-hist}) and (\ref{mean-hist}) are different in shapes. \\
  
{\bf Answer:} The shape of the gamma distribution with parameters shape = 5, scale = 5 is right skewed which we can see in the histogram of question (\ref{exp-hist}). But shape of the distribution of the mean is symmetric like normal as we see in question (\ref{mean-hist}). This is a fact we learn from central limit theorem. The theorem states that mean has a normal distribution no mater what the original distribution is. Suppose we have $$X \sim gamma(k,\theta)$$ where $k=5, \theta = 5$, the mean of the distribution would be $\mu = k\theta = 25$. The variance of $X$ would be $\sigma^2=k \theta^2 = 125$. The central limit theorem states that $$\sqrt{n}(\bar{X}-\mu)/\sigma \rightarrow N(0,1)$$ which is approximately equivalent to saying for large sample size $n$ $$\bar X \sim N(\mu, \sigma^2 /n).$$ This is exactly what is happening here. Each of the rows in the matrix is a sample of size $n=$ \Sexpr{ncol(myMatrix)} from gamma distribution. The histogram of row means shows a symmetric bell shape. The mean of the means ($\bar X$) is \Sexpr{mean(rowmeans)} (close to $\mu$) and the variance of the row means is \Sexpr{var(rowmeans)} which is very similar to $\sigma^2/n = 125/100$. 
  
\end{enumerate}


\item What are the very first few steps one should do once data is loaded onto \textbf{R}? Demonstrate that by loading tips data from http://www.ggobi.org/book/data/tips.csv \\

{\bf Answer:} The first few steps are focusing on learning about the data that is just loaded, such as what are the variables, what types of data are loaded, how the data look. We can even think of using a basic \texttt{plot()} command, but we have to be very careful about using it. It may not always be very useful. Also for large number of variables it will take long time to run and we will not be able to view anything. In that case we have to rely on what we see from other commands such as \texttt{head()}. Then we can give a plot command with some selected variables that will make more sense. The following codes and the results demonstrate the process.

<<fig.width=8, fig.height=8, fig.align='center',out.width='.85\\linewidth'>>=
dat <- read.csv("http://www.ggobi.org/book/data/tips.csv")
head(dat)
tail(dat)
summary(dat)
str(dat)
plot(dat[,c(1,2,7)])
@
We can think of a more useful plot command. For this we need to install package \texttt{GGally}. But be cautious, it will take time to be executed. Notice the type of plots it created based on the variable types. What do you see? Does it show the relationship between categorical variables as well? Could we see that from our previous plot?

<<fig.width=10, fig.height=10, fig.align='center',out.width='.99\\linewidth', message=FALSE>>=
library(GGally)
ggpairs(dat[,-c(1)], colour='day', alpha=0.4)
@ 



\item We don't usually want to write loops in R. Answer the following questions to avoid loops. (optional for undergraduate students, mandatory for graduate students).

\begin{enumerate}
\item Explain what the following R codes are doing.

<<>>=
myMatrix <- matrix(1:12, nrow=3)
output <- NULL
for (i in 1:3){
  output <- c(output, sum(myMatrix[i,]))
}
output
@

{\bf Answer:} These R codes first provide a matrix called `myMatrix' of numbers from 1 through 12 with 3 rows and 4 columns. Later, using a loop the codes provide the sums of all the row values of `myMatrix'.

\item Provide R codes that will produce the same result without using loop

<<>>=
output <- apply(myMatrix,1,sum)
output
@


\end{enumerate}



\end{enumerate}


{\bf What we learnt from this homework:}
\begin{enumerate}
  \item We have done our first reproducible work
  \item When we write numerical summary of the data in text we don't write the exact number instead write the \textbf{R} code. This helps if data are changed and the new summary will be updated automatically.
  \item When we write \textbf{R} functions we have to be very specific what data type that function would take as an argument. We need to know our function to use it appropriately.
  \item To maintain the reproducibility, specify the seed when we want to simulate data
  \item We can generate many samples and calculate their statistics without using loops
  \item We just demonstrated and witnessed that central limit theorem works
  \item We learnt how to write math equation in our report
  \item When we plot, we don't plot blindly. We explore the data first, we learn about the data type and we give a smart plot command that produces useful plots.
  \item How to resize the picture in the document so that it looks cute and professional
  \item For some of us, the technical issues of reproducibility was not straight forward. This happens with the data scientists. We have to spend many hours with tiny errors or typos in the code. So, the learning is, start working on the HW from the day one. The upcoming homeworks may be time consuming.
\end{enumerate}


\end{document}